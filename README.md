# LT2212 V19 Assignment 3

From Asad Sayeed's statistical NLP course at the University of Gothenburg.

My name: Britta Carlsson

## Additional instructions

I think the scripts gendoc.py and train.py works okay. I haven't done error handling. In test.py, there may be something wrong with the calculation of perplexity, I got a very large value. I tried to test scripts with larger training inputs, but it never finished. I unfortunetaly don't have time to fix this, cause I'm also behind with other stuff. I guess my hypothesis would be that accuracy would be greater and perplexity lower with more training data. 3-grams should be ideal, since bigger n-grams than that would become hard to find in the data (sparsity problem), and bigrams would be less accurate in predicting next word (too many to choose from). 


